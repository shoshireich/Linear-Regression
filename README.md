# Linear-Regression
## Abstract
Given a dataset with no apparent relationship between x-values and y-values, the ability of scipy.optimize.curve_fit to create a best-fit line is evaluated. In linear regression, it is assumed that the data are related by a linear model y=βx+e, where β is the regression coefficient and e is Gaussian noise. By generating many datasets where there is no relationship between X and Y (β = 0) and fitting a line to each simulation, it is possible to estimate the likelihood of seeing the relationship generated by scipy.optimize.curve_fit in the original dataset by chance. The regression coefficient for the given dataset is compared to a distribution of regression coefficients from running many simulations. Two different distributions are created for comparison. The first involves best-fit lines of completely new generated datasets, while the second involves generating new datasets through bootstrapping. The regression coefficient from cipy.optimize.curve_fit is shown to be almost equal to the median regression coefficent from the distribution. Finally, a distribution of residuals from the original best-fit line created by scipy.optimize.curve_fit is created. This distribution is found to somewhat follow a normal distribution.

## Methods
First, scipy.optimize.curve_fit is used to form a best-fit line for the original dataset. The output covariance of the fit is used to determine the 95% confidence interval in the slope of the fitted line. Then, 1000 new datasets with no relationship between X and Y are generated. The values in each dataset are randomly picked between the lower and upper bounds of x- and y-values from the original dataset. A distribution of regression coefficients is created from these new datasets and used to evaluate the probability of obtaining the scipy.optimize.curve_fit result from the original dataset. This process of generating new datasets, fitting a line to each dataset, and comparing the distribution of slopes to the original result is repeated. However, this time the new datasets are generated through bootstrapping. The original data is resampled with replacement to create 1000 new datasets. The 2.5th percentile and 97.5th percentiles of the bootstrapped distribution are calculated, so that 95% of the regression coefficients fall between these two values. This range is used to compare to the 95% confidence interval in the regression coefficient of the original dataset. 

In addition to regression coefficients, the residuals are also evaluated. The residuals are calculated by subtracting the original data from the scipy.optimize.curve_fit fitted model. The Kolmogorov-Smirnov test is used to determine how well the distribution of residuals follows a normal distribution. 

## Results and Discussion
The regression coefficient found by scipy.optimize.curve_fit is: 0.0741

The 95% confidence interval in the fitted slope of the line is: [0.0457, 0.102] 


The distribution of regression coefficients from datasets generated randomly indicates that the likelihood of seeing a relationship with slope greater than or equal to the best-fit coefficient of the original dataset is 6.60%. However, bootstrapping is perhaps a better way to assess confidence in the scipy.optimize.curve_fit results. When comparing to the distribution of regression coefficients of bootstrapped datasets, the likelihood of seeing a regression coefficient less than or equal to the best-fit coefficient of the original dataset is 48.30%. These results are shown below in Figure 1. 
<img src="https://github.com/shoshireich/Linear-Regression/blob/master/Figures/Figure1.png" width="75%">
 
The median regression coefficient from the distribution of bootstrapped datasets is: 0.0733

The 2.5th percentile is: 0.0234

The 97.5th percentile is: 0.122

Figure 2 shows how these values compare to the scipy.optimize.curve_fit best-fit of the original dataset. As seen below, it appears as though the scipy.optimize.curve_fit best-fit line and the line using the median regression coefficient from the bootstrapped datasets are almost equal. However, the bootstrapped model has a greater confidence interval. This makes sense considering the wide spread of the data.
<img src="https://github.com/shoshireich/Linear-Regression/blob/master/Figures/Figure2.png" width="75%">

The Kolmogorov-Smirnov statistic is: 0.268

The probability of observing residuals is: 7.673e-07

The Kolmogorov-Smirnov statistic suggests that the normal distribution is an 'okay' though not great fit for the residuals. The probability of observing residuals is extremely low, which may explain the deviation from what is expected.
Figure 3 shows the distribution of residuals. 
